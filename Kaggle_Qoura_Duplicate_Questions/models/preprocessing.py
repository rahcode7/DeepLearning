# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pj_oPmk-RfO4Fzz3J3B-n2_tQEW_l6gA
"""

from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelBinarizer
import os
import pickle

class preprocessing():
  def __init__(self,train,test):
    print("class instantiated")
    self.maxlen = 100
    self.vocab_size = 3000 #10000

    #self.embed_size = 300
    self.split_size = 0.10  # For 90/10 train/val split
    self.train = train
    self.test = test
    self.train['question12'] = train['question1'] +str(" ") + train['question2']
    self.test['question12'] = test['question1'] +str(" ") + test['question2']
    self.training,self.val = train_test_split(train, test_size=self.split_size, random_state=10)

  # Encode Fn - Return encoded train,val,test i/p datasets
  def encode(self):

    #train , test = self.concatenate_strings(train,test)
    t = Tokenizer(num_words=self.vocab_size)


    train_X = self.training["question12"].fillna("##").values
    val_X = self.val["question12"].fillna("##").values
    test_X = self.test['question12'].fillna("##").values

    print("Encoding...")
    # Fit on docs
    t.fit_on_texts(train_X)

    # Vectorise each test documents based on dictionary
    index_list = t.texts_to_sequences(train_X)

    # Pad Zeros
    training_X = pad_sequences(index_list, maxlen=self.maxlen)
    #print(training_X.shape)

    # Vectorise each test documents based on dictionary
    index_list_test = t.texts_to_sequences(test_X)
    test_X_encoded = pad_sequences(index_list_test, maxlen=self.maxlen)

    # Vectorise validation sen
    index_list_test = t.texts_to_sequences(val_X)
    val_X_encoded = pad_sequences(index_list_test, maxlen=self.maxlen)

    train_y = self.train['is_duplicate'].values
    val_y = self.val['is_duplicate'].values


    return(training_X,train_y,val_X_encoded,val_y,test_X_encoded)


from data_loader.data_loader import data_loader
data = data_loader()
train = data.get_train_data()
test =  data.get_test_data()
print("Data loaded - Training ",train.shape)
print("Data loaded - Test ",test.shape)

p = preprocessing(train,test)
training_X,train_y,val_X_encoded,val_y, test_X_encoded = p.encode()

#print(os.path.join(os.getcwd(),'models/files/training_X.pickle'))

with open(os.path.join(os.getcwd(),'models/files/training_X.pickle'), 'wb') as f:
    pickle.dump(training_X, f)
f.close()
#
with open('models/files/train_y.pickle', 'wb') as f:
    pickle.dump(train_y, f)
f.close()
with open('models/files/val_X_encoded.pickle', 'wb') as f:
    pickle.dump(val_X_encoded, f)
f.close()
with open('models/files/val_y.pickle', 'wb') as f:
    pickle.dump(val_y, f)
f.close()
with open('models/files/test_X_encoded.pickle', 'wb') as f:
    pickle.dump(test_X_encoded,  f)
f.close()
print("Data Preprocessed and saved",(val_X_encoded.shape,val_y.shape))
